{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#07404E\"><b>Pandas</b><br></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is an open-source Python library primarily used for data manipulation and analysis. It provides powerful data structures<br> like DataFrame and Series that allow you to work with structured data efficiently, enabling tasks such as data cleaning, transformation, and aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Reasons for Using Pandas:<br>\n",
    "\n",
    "<font color=\" #0099322\"><b>Data Handling Capabilities:</b><br></font>\n",
    "\n",
    "Pandas makes it easy to load, clean, and process large datasets from various formats (e.g., CSV, Excel, SQL databases).<br>\n",
    "It provides intuitive and powerful ways to manipulate tabular data (like Excel but with much more flexibility in Python).\n",
    "\n",
    "<b></b></br>\n",
    "<font color=\" #0099322\"><b>Data Structures: Series and DataFrame:</b><br></font>\n",
    "\n",
    "Series: A one-dimensional array-like structure that can hold data of any type (like a list or a column in Excel).<br>\n",
    "DataFrame: A two-dimensional, table-like data structure with labeled axes (rows and columns). Think of it like an advanced spreadsheet that makes data manipulation intuitive.\n",
    "\n",
    "<b></b><br>\n",
    "<font color=\" #0099322\"><b>Easy Data Cleaning:</b><br></font>\n",
    "\n",
    "Pandas simplifies common data cleaning tasks like handling missing data, filtering, and transforming data. You can easily clean messy datasets to make them ready for analysis.\n",
    "\n",
    "<b></b><br>\n",
    "<font color=\" #0099322\"><b>Efficient and Fast:</b><br></font>\n",
    "\n",
    "Built on top of NumPy, Pandas is fast and optimized for performance. It can handle large datasets efficiently.\n",
    "\n",
    "<b></b><br>\n",
    "<font color=\" #0099322\"><b>Data Analysis and Aggregation:</b><br></font>\n",
    "\n",
    "With Pandas, you can perform complex data analysis using operations like grouping, merging, and aggregation in just a few lines of code.<br> This is useful for summarizing and analyzing big datasets.\n",
    "\n",
    "<b></b><br>\n",
    "<font color=\" #0099322\"><b>Integration with Other Libraries:</b><br></font>\n",
    "\n",
    "Pandas works seamlessly with other Python libraries like NumPy, Matplotlib, and Scikit-learn, making it a great tool for <br> data analysis and machine learning workflows.\n",
    "\n",
    "<b></b><br>\n",
    "<font color=\" #0099322\"><b>Plotting Capabilities:</b><br></font>\n",
    "\n",
    "It has built-in plotting functions (through Matplotlib) to visualize data easily.\n",
    "\n",
    "<b></b><br>\n",
    "<font color=\" #0099322\"><b>Flexibility:</b><br></font>\n",
    "\n",
    "Pandas supports various data formats, such as CSV, Excel, SQL databases, JSON, etc., and can transform data between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>\n",
    "<font color=\"#C68D32\"><b>Prerequisites:</b><br></font>\n",
    "\n",
    "Basic Python Programming:<br>\n",
    "  <b>----></b> Variables, data types, loops, functions.\n",
    "\n",
    "NumPy:<br>\n",
    "  <b>----></b> Arrays, array operations, basic functions.\n",
    "\n",
    "Understanding Tabular Data:<br>\n",
    "  <b>----></b> Tables, rows, columns, and CSV file formats.\n",
    "\n",
    "File Handling:<br>\n",
    "  <b>----></b> Reading and writing files in Python, especially CSV files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color=\"#C68D32\"><b>Step-by-Step Instructions:</b><br></font>\n",
    "\n",
    "\n",
    "Open Terminal/Command Prompt<br>\n",
    "Run the pip install Command Type the following command to install Pandas<br>\n",
    "-----------pip install pandas-----------<br>\n",
    "Verify Installation:<br>\n",
    "-----------import pandas as pd-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas sample code for reading data from a CSV file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read data from a CSV file\n",
    "df = pd.read_csv('Details.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#30AF5D\"><b>Why we have to use Pandas why not excel?</b><br></font>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "1.User Interface<br>\n",
    "2.Data Handling Capabilities<br>\n",
    "3.Automation and Reproducibility<br>\n",
    "4.Complexity and Learning Curve<br>\n",
    "5.Data Visualization<br>\n",
    "6.Data Cleaning and Transformation<br>\n",
    "7.Collaboration<br>\n",
    "8.Error Handling and Debugging<br>\n",
    "9.Integration with Other Tools<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a Series from a list\n",
    "s = pd.Series([10, 20, 30, 40, 50])\n",
    "print(s)\n",
    "\n",
    "# Creating a Series with custom index labels\n",
    "s_custom_index = pd.Series([100, 200, 300], index=['a', 'b', 'c'])\n",
    "print(s_custom_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"#CC5A0F\"><b>Types of Data</b><br></font>\n",
    "\n",
    "<font color=\"#0BBBC2\"><b>Structured Data (Main Focus of Pandas)</b><br></font>\n",
    "\n",
    "Structured data refers to data that is organized in a clear, predefined format, usually in rows and columns. Examples include:<br>\n",
    "\n",
    "CSV files<br>\n",
    "Excel spreadsheets<br>\n",
    "SQL databases<br>\n",
    "DataFrames (2D data structures)<br>\n",
    "\n",
    "Pandas is designed specifically to handle structured data through its two core data structures:<br>\n",
    "\n",
    "<b>Series (1D):</b> A single column of data, similar to a list or array.<br>\n",
    "<b>DataFrame (2D):</b> A table with rows and columns, similar to a database table or Excel spreadsheet.<br>\n",
    "\n",
    "Using Pandas we can easily perform:<br>\n",
    "Data manipulation (filtering, sorting, reshaping, merging, etc.).<br>\n",
    "Statistical operations (mean, median, sum, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color=\"#0BBBC2\"><b>Semi-Structured Data (Supported to Some Extent)</b><br></font>\n",
    "\n",
    "JSON (JavaScript Object Notation) files: These files have nested structures, and Pandas can flatten these structures into tabular form.<br>\n",
    "XML (eXtensible Markup Language): Similar to JSON, XML data can also be parsed and transformed into a structured format.<br>\n",
    "\n",
    "Pandas has built-in functions: <br>\n",
    "\n",
    "<b>pd.read_json():</b> Reads JSON data and converts it into a DataFrame.<br>\n",
    "<b>pd.read_xml():</b> Reads XML data and converts it into a DataFrame (available in recent versions of Pandas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color=\"#0BBBC2\"><b>Unstructured Data (Limited Support)</b><br></font>\n",
    "\n",
    "Unstructured data refers to data that doesn’t have any predefined format or structure. Examples include:\n",
    "\n",
    "Text files (plain text, emails, articles)\n",
    "Images, videos, and audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"#CC5A0F\"><b>Understanding Data Structures</b><br></font>\n",
    "\n",
    "<b>Series (1D)</b><br>\n",
    "\n",
    "<b>----></b> A Series is essentially a one-dimensional labeled array capable of holding any data type (integers, strings, floats, Python objects, etc.). It's like a column in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Series\n",
    " \n",
    "import pandas as pd\n",
    "\n",
    "# Creating a Series from a list\n",
    "s = pd.Series([10, 20, 30, 40, 50])\n",
    "print(s)\n",
    "\n",
    "# Creating a Series with custom index labels\n",
    "s_custom_index = pd.Series([100, 200, 300], index=['a', 'b', 'c'])\n",
    "print(s_custom_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>DataFrame (2D)</b><br>\n",
    "\n",
    "<b>----></b> A DataFrame is a two-dimensional labeled data structure, similar to a table or spreadsheet. It consists of rows and columns, where each column can be a different data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame from a dictionary\n",
    "\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### <font color=\"#39A40A\"><b>Creating Data Structures</b><br></font>\n",
    "\n",
    "Pandas makes it easy to create Series and DataFrames from various data types like lists, dictionaries, and NumPy arrays.<br>\n",
    "\n",
    "<b>Creating a Series</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a list:\n",
    "import pandas as pd\n",
    "s = pd.Series([1, 2, 3, 4])\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From a dictionary:\n",
    "\n",
    "s_dict = pd.Series({'a': 100, 'b': 200, 'c': 300})\n",
    "print(s_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From a NumPy array:\n",
    "\n",
    "import numpy as np\n",
    "np_array = np.array([10, 20, 30])\n",
    "s_array = pd.Series(np_array)\n",
    "print(s_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>\n",
    "##### <font color=\"#39A40A\"><b>Creating a DataFrame</b><br></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From a list of lists (or 2D lists):\n",
    "\n",
    "df_list = pd.DataFrame([[1, 'Alice', 'HR'], [2, 'Bob', 'Engineering']])\n",
    "print(df_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From a dictionary:\n",
    "\n",
    "data_dict = {\n",
    "    'Name': ['Tom', 'Jerry', 'Mickey'],\n",
    "    'Age': [23, 25, 30],\n",
    "    'Salary': [50000, 60000, 75000]\n",
    "}\n",
    "df_dict = pd.DataFrame(data_dict)\n",
    "print(df_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From a NumPy array:\n",
    "\n",
    "np_data = np.array([[1, 'Alice'], [2, 'Bob'], [3, 'Charlie']])\n",
    "df_np = pd.DataFrame(np_data, columns=['ID', 'Name'])\n",
    "print(df_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>\n",
    "##### <font color=\"#39A40A\"><b>Attributes of DataFrames</b><br></font> \n",
    "\n",
    "Pandas DataFrames have several useful attributes that provide metadata about the data, including shape, index, columns, and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape: Returns the shape (number of rows and columns) of the DataFrame.\n",
    "\n",
    "print(df.shape)  # Output: (3, 3) -> 3 rows, 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index: Shows the index (row labels) of the DataFrame.\n",
    "print(df.index)  # Output: RangeIndex(start=0, stop=3, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns: Lists all the column labels of the DataFrame.\n",
    "print(df.columns)  # Output: Index(['Name', 'Age', 'City'], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes: Returns the data type of each column.\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <font color=\"#CC5A0F\"><b> DataFrame Operations</b><br></font>  \n",
    "\n",
    "Pandas allows you to manipulate and analyze data efficiently through various operations on DataFrames. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>\n",
    "##### <font color=\"#39A40A\"><b>Indexing and Selecting Data</b><br></font> \n",
    "\n",
    "Pandas provides several ways to index and select data from a DataFrame. The most common methods are loc[] and iloc[], along with direct column indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>loc[] for Label-based Indexing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35], 'Salary': [50000, 60000, 75000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Accessing a single row by index label\n",
    "print(df.loc[0])  # Row where index = 0\n",
    "print(\"----------------------------------------------------------------\")\n",
    "# Accessing a single value (row 1, column 'Salary')\n",
    "print(df.loc[1, 'Salary'])\n",
    "print(\"----------------------------------------------------------------\")\n",
    "# Accessing multiple rows and columns\n",
    "print(df.loc[0:2, ['Name', 'Salary']])  # Slices the first 3 rows and selects columns 'Name' and 'Salary'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>iloc[] for Position-based Indexing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing a single row by position\n",
    "print(df.iloc[0])  # First row\n",
    "print(\"----------------------------------------------------------------\")\n",
    "# Accessing a specific value by position (row 1, column 2)\n",
    "print(df.iloc[1, 2])  # Value at row 1, column 2\n",
    "print(\"----------------------------------------------------------------\")\n",
    "# Accessing a range of rows and specific columns by position\n",
    "print(df.iloc[0:2, 0:2])  # Slices first 2 rows and first 2 columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly access a column by its name like you would access a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a column as a Series\n",
    "print(df['Name'])\n",
    "print(\"----------------------------------------------------------------\")\n",
    "# Access multiple columns\n",
    "print(df[['Name', 'Salary']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Based on Labels vs. Based on Integer Positions<br>\n",
    "\n",
    "<b>loc[]:</b><br>\n",
    "----> Accesses data by label (the index or column name).<br>\n",
    "----> Uses the explicit labels of rows and columns (row labels and column names).<br>\n",
    "----> It's inclusive for both rows and columns (meaning if you specify a range, the last item will be included).<br>\n",
    "\n",
    "<b>iloc[]:</b><br>\n",
    "----> Accesses data by integer position (the numerical index of rows or columns).<br>\n",
    "----> Uses integer positions, similar to how arrays or lists are indexed in Python (starting from 0).<br>\n",
    "----> It's exclusive for the end of a range (like Python slicing, where the stop value is not included).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one is faster.?<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<b>iloc[]</b> operates on integer positions, which are the internal structure of the DataFrame. Since it directly references the row and column positions, there's no need to look up labels.<br>\n",
    "\n",
    "<b>loc[]</b> operates on labels (row index names or column names). When you use .loc[], Pandas has to match the label to the corresponding position in the DataFrame,<br> which can add a small overhead for the label lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance Example:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Create a large DataFrame\n",
    "dff = pd.DataFrame(np.random.randn(1000000, 6), columns=list('ABCDEF'))\n",
    "\n",
    "# Using .loc[] to access the first row\n",
    "start_loc = time.time()\n",
    "dff.loc[0]\n",
    "end_loc = time.time()\n",
    "\n",
    "# Using .iloc[] to access the first row\n",
    "start_iloc = time.time()\n",
    "dff.iloc[0]\n",
    "end_iloc = time.time()\n",
    "\n",
    "print(f\"loc[] took: {end_loc - start_loc} seconds\")\n",
    "print(f\"iloc[] took: {end_iloc - start_iloc} seconds\")\n",
    "\n",
    "# Note: The actual performance difference might be negligible for small DataFrames, but for very large DataFrames, .iloc[] can be faster because it doesn't involve label lookup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### <font color=\"#CC5A0F\"><b>Column and Row Manipulation</b><br></font> \n",
    "\n",
    "Pandas makes it easy to add, rename, and drop columns or rows in a DataFrame.<br>\n",
    "\n",
    "<b>Adding Columns</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new column 'Bonus'\n",
    "df['Bonus'] = [5000, 6000, 7000]\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Renaming Columns</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns 'Age' to 'Years', 'Salary' to 'Income'\n",
    "df_renamed = df.rename(columns={'Age': 'Years', 'Salary': 'Income'})\n",
    "print(df_renamed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Dropping Columns or Rows</b><br>\n",
    "\n",
    "<b>----></b> Use drop() to remove columns or rows.<br>\n",
    "<b>----></b> For columns: set axis=1, and for rows: use axis=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping a column 'Bonus'\n",
    "df_no_bonus = df.drop('Bonus', axis=1)\n",
    "print(df_no_bonus)\n",
    "print(\"----------------------------------------------------------------\")\n",
    "# Dropping a row by index (removing row 1)\n",
    "df_no_row_1 = df.drop(1, axis=0)\n",
    "print(df_no_row_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"#CC5A0F\"><b>Basic Operations on DataFrames</b><br></font> \n",
    "\n",
    "<b>----></b> You can perform arithmetic operations on DataFrame columns, and Pandas automatically aligns them by row and column labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6],\n",
    "    'C': [7, 8, 9]\n",
    "})\n",
    "\n",
    "# Addition of a scalar (adds 10 to every element in the DataFrame)\n",
    "df_add = df + 10\n",
    "print(df_add)\n",
    "print(\"----------------------------------------------------------------\")\n",
    "# Subtraction between columns\n",
    "df_sub = df['B'] - df['A']\n",
    "print(df_sub)\n",
    "print(\"----------------------------------------------------------------\")\n",
    "# Element-wise multiplication between two columns\n",
    "df_mul = df['A'] * df['C']\n",
    "print(df_mul)\n",
    "print(\"----------------------------------------------------------------\")\n",
    "# Adding two DataFrames\n",
    "df2 = pd.DataFrame({\n",
    "    'A': [10, 20, 30],\n",
    "    'B': [40, 50, 60],\n",
    "    'C': [70, 80, 90]\n",
    "})\n",
    "\n",
    "df_sum = df + df2\n",
    "print(df_sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Handling Missing Data</b><br> \n",
    "\n",
    "<b>----> </b>If your DataFrame contains NaN (Not a Number) values, arithmetic operations will treat them as missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_nan = pd.DataFrame({\n",
    "    'X': [1, 2, None],\n",
    "    'Y': [4, None, 6]\n",
    "})\n",
    "\n",
    "# Adding two columns, with NaN values in between\n",
    "df_sum_nan = df_with_nan['X'] + df_with_nan['Y']\n",
    "print(df_sum_nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:</b>Handling missing data is a crucial step in data preprocessing. Pandas provides several tools to identify and handle missing data effectively. <br>\n",
    "\n",
    "<b>Identifying Missing Data</b><br> \n",
    "\n",
    "<b>----></b> Missing data is typically represented as NaN (Not a Number). You can use functions like isnull(), notnull(), and sum() to identify where these missing values are located in a DataFrame.\n",
    "\n",
    "<b>isnull()  : </b> Returns a DataFrame of the same shape as the original, but with Boolean values. True indicates missing values.<br>\n",
    "<b>notnull() :</b> Returns the opposite of isnull()—True indicates that the value is not missing.<br>\n",
    "<b>sum()     :</b> Can be used after isnull() or notnull() to count the number of missing or non-missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {'A': [1, 2, np.nan, 4],\n",
    "        'B': [5, np.nan, np.nan, 8],\n",
    "        'C': [10, 11, 12, np.nan]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Identifying missing values\n",
    "print(df.isnull())\n",
    "print(\"----------------------------------------------------------------\")\n",
    "# Counting the number of missing values per column\n",
    "print(df.isnull().sum())\n",
    "print(\"----------------------------------------------------------------\")\n",
    "# Identifying non-missing values\n",
    "print(df.notnull())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>----> </b> Once you’ve identified missing data, you can handle it by either dropping the missing values or filling them with appropriate replacements.<br>\n",
    "##### <font color=\"#CC5A0F\"><b>Dropping Missing Values: dropna()</b><br></font> \n",
    "\n",
    "<b>----></b>This function removes rows or columns containing missing values<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with any missing values\n",
    "\n",
    "df_dropped_rows = df.dropna()\n",
    "print(df_dropped_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns with any missing values\n",
    "\n",
    "df_dropped_cols = df.dropna(axis=1)\n",
    "print(df_dropped_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Filling Missing Values: fillna()<br></b>\n",
    "\n",
    "<b>----> </b> Instead of removing missing data, you can fill them using methods like forward-fill (ffill), backward-fill (bfill), or filling with specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values with a specific value (e.g., 0)\n",
    "df_filled = df.fillna(0)\n",
    "print(df_filled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill (propagates the last valid observation forward)\n",
    "df_ffill = df.fillna(method='ffill')\n",
    "print(df_ffill)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward fill (propagates the next valid observation backward)\n",
    "df_bfill = df.fillna(method='bfill')\n",
    "print(df_bfill)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Imputation Techniques</b><br>\n",
    "\n",
    "<b>----> </b> Imputation is a technique where you replace missing values with a statistical value like the mean, median, or mode of the column.<br> These methods are particularly useful when the missing values are not random.\n",
    "\n",
    "<b>Imputation with Mean<br></b>\n",
    "\n",
    "----> You replace missing values with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation with mean\n",
    "mean_value = df['A'].mean()\n",
    "df['A'] = df['A'].fillna(mean_value)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Imputation with Median<br></b>\n",
    "\n",
    "<b>----> </b>The median is a better option when the data contains outliers, as the median is less sensitive to extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation with median\n",
    "median_value = df['B'].median()\n",
    "df['B'] = df['B'].fillna(median_value)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Imputation with Mode<br></b>\n",
    "\n",
    "<b>----></b> The mode is the most frequent value in the column. This is useful for categorical columns where you want to replace missing values with the most common category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation with mode\n",
    "mode_value = df['C'].mode()[0]\n",
    "df['C'] = df['C'].fillna(mode_value)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Removing Duplicates</b><br>\n",
    "\n",
    "<b>----></b> In a dataset, you might encounter duplicate rows, which can distort the analysis. Pandas provides the drop_duplicates() method to identify and remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with duplicates\n",
    "data = {'Name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob'],\n",
    "        'Age': [25, 30, 25, 35, 30],\n",
    "        'Salary': [50000, 60000, 50000, 70000, 60000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Removing duplicate rows\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "print(df_no_duplicates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>----> </b> Removing Duplicates Based on Specific Columns\n",
    "You can also remove duplicates based on specific columns using the subset parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates based only on the 'Name' column\n",
    "df_unique_names = df.drop_duplicates(subset=['Name'])\n",
    "print(df_unique_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Common String Operations:</b><br>\n",
    "\n",
    "<b>str.strip(): </b>Removes leading and trailing spaces.<br>\n",
    "<b>str.lower():</b> Converts all characters in the string to lowercase.<br>\n",
    "<b>str.contains():</b> Checks whether the column contains a substring or pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with inconsistent string formatting\n",
    "data = {'Name': [' Alice', 'BOB', 'Charlie ', 'alice'],\n",
    "        'City': [' New York', 'LONDON ', 'tokyo', 'New York']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Removing leading/trailing spaces using str.strip()\n",
    "df['Name'] = df['Name'].str.strip()\n",
    "\n",
    "# Converting to lowercase using str.lower()\n",
    "df['Name'] = df['Name'].str.lower()\n",
    "df['City'] = df['City'].str.lower()\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Checking if 'City' contains the word 'new'\n",
    "df['Is_New_City'] = df['City'].str.contains('new')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data Type Conversion</b><br>\n",
    "\n",
    "<b>----></b>Sometimes, columns may have incorrect data types (e.g., numbers stored as strings). You can convert data types using the astype() method in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with mixed data types\n",
    "data = {'Age': ['25', '30', '35', '40'],  # Stored as strings\n",
    "        'Salary': [50000, 60000, 70000, 80000]}  # Stored as integers\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Converting 'Age' from string to integer\n",
    "df['Age'] = df['Age'].astype(int)\n",
    "print(df.dtypes)  # Check the data types\n",
    "\n",
    "# Converting 'Salary' from integer to float\n",
    "df['Salary'] = df['Salary'].astype(float)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Replacing Values</b><br>\n",
    "\n",
    "<b>----></b> In many cases, datasets can contain inconsistent or erroneous values (e.g., typos, outliers). You can use the replace() function to substitute those values with more appropriate ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with inconsistent values\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie', 'Charlie', 'Bob'],\n",
    "        'Gender': ['Female', 'M', 'Male', 'Male', 'M']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Replacing inconsistent gender codes (e.g., 'M' -> 'Male')\n",
    "df['Gender'] = df['Gender'].replace({'M': 'Male', 'F': 'Female'})\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data Transformation<br></b><br>\n",
    "\n",
    "<b>----> </b> Data transformation is a key step in preparing data for analysis or modeling. It includes operations like renaming columns, mapping specific values in columns, and handling categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Renaming Columns</b><br>\n",
    "\n",
    "<b>----> </b> Renaming columns is useful when column names are not descriptive or need to be standardized. Pandas provides the rename() method for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'A': [1, 2, 3],\n",
    "        'B': [4, 5, 6],\n",
    "        'C': [7, 8, 9]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Renaming columns\n",
    "df_renamed = df.rename(columns={'A': 'Age', 'B': 'Salary', 'C': 'Score'})\n",
    "print(df_renamed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'A': 'Age', 'B': 'Salary', 'C': 'Score'}, inplace=True)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Mapping Values</b><br>\n",
    "\n",
    "<b>----> </b> Sometimes, specific values in a column need to be replaced or mapped to new values. This is common in scenarios where you need to clean or reclassify data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie', 'Alice'],\n",
    "        'Gender': ['Female', 'Male', 'Male', 'Female']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mapping values: changing 'Female' to 0 and 'Male' to 1\n",
    "gender_map = {'Female': 0, 'Male': 1}\n",
    "df['Gender_Mapped'] = df['Gender'].map(gender_map)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>----></b> Similar to map(), replace() is used to substitute specific values in a DataFrame or Series. It works for both single and multiple replacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing values in the 'Gender' column\n",
    "df['Gender'] = df['Gender'].replace({'Female': 'F', 'Male': 'M'})\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Merging and Joining\n",
    "Data merging and joining is a critical part of data wrangling, especially when working with multiple datasets. Pandas provides powerful tools like concat() for concatenation, merge() for joining DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". Concatenation\n",
    "Concatenation involves stacking DataFrames either vertically (row-wise) or horizontally (column-wise). Pandas provides the concat() function to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrames\n",
    "df1 = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Age': [25, 30]})\n",
    "df2 = pd.DataFrame({'Name': ['Charlie', 'David'], 'Age': [35, 40]})\n",
    "\n",
    "# Vertical concatenation (adding rows)\n",
    "df_vertical = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "print(df_vertical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrames\n",
    "df1 = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Age': [25, 30]})\n",
    "df2 = pd.DataFrame({'Salary': [50000, 60000], 'Department': ['HR', 'IT']})\n",
    "\n",
    "# Horizontal concatenation (adding columns)\n",
    "df_horizontal = pd.concat([df1, df2], axis=1)\n",
    "print(df_horizontal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Merging and Joining DataFrames\n",
    "Merging and joining are used to combine DataFrames based on common columns (keys). The Pandas merge() function provides several ways to perform SQL-like joins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combines two DataFrames on a key (or multiple keys).\n",
    "Supports different types of joins: inner, outer, left, and right.\n",
    "Types of Joins:\n",
    "Inner Join: Returns only matching rows from both DataFrames.\n",
    "Outer Join: Returns all rows, filling with NaN where no match is found.\n",
    "Left Join: Returns all rows from the left DataFrame and matching rows from the right.\n",
    "Right Join: Returns all rows from the right DataFrame and matching rows from the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrames\n",
    "df1 = pd.DataFrame({'ID': [1, 2, 3], 'Name': ['Alice', 'Bob', 'Charlie']})\n",
    "df2 = pd.DataFrame({'ID': [1, 2, 4], 'Salary': [50000, 60000, 70000]})\n",
    "\n",
    "# Inner join on 'ID'\n",
    "df_inner = pd.merge(df1, df2, on='ID', how='inner')\n",
    "print(df_inner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join on 'ID'\n",
    "df_left = pd.merge(df1, df2, on='ID', how='left')\n",
    "print(df_left)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join on 'ID'\n",
    "df_outer = pd.merge(df1, df2, on='ID', how='outer')\n",
    "print(df_outer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Duplicates in Merging\n",
    "When merging DataFrames, duplicates or overlapping data can arise, particularly when you have repeated values in the keys. There are strategies to handle this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrames with duplicate 'ID' values\n",
    "df1 = pd.DataFrame({'ID': [1, 2, 2, 3], 'Name': ['Alice', 'Bob', 'Bob', 'Charlie']})\n",
    "df2 = pd.DataFrame({'ID': [1, 2, 4], 'Salary': [50000, 60000, 70000]})\n",
    "\n",
    "# Inner join\n",
    "df_merged = pd.merge(df1, df2, on='ID', how='inner')\n",
    "\n",
    "# Dropping duplicates\n",
    "df_cleaned = df_merged.drop_duplicates(subset=['ID', 'Salary'])\n",
    "print(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting and Filtering Data\n",
    "Sorting and filtering data are essential operations for exploring and analyzing datasets. Pandas provides several ways to sort and filter data, as well as an efficient query() method for querying data based on conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting Data\n",
    "Sorting is a way to reorder the data in a DataFrame based on column values or the index. Pandas provides two main methods for sorting: sort_values() and sort_index()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by Column Values\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "        'Age': [25, 30, 35, 40],\n",
    "        'Salary': [50000, 60000, 70000, 80000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sorting by 'Age' in ascending order\n",
    "df_sorted_by_age = df.sort_values(by='Age')\n",
    "print(df_sorted_by_age)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting by Multiple Columns\n",
    "\n",
    "# Sorting by 'Age' (ascending) and 'Salary' (descending)\n",
    "df_sorted_multi = df.sort_values(by=['Age', 'Salary'], ascending=[True, False])\n",
    "print(df_sorted_multi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting by Index\n",
    "\n",
    "# Sorting by index (ascending)\n",
    "df_sorted_by_index = df.sort_index()\n",
    "print(df_sorted_by_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering Data\n",
    "Filtering allows you to subset a DataFrame based on conditions, such as selecting rows where a column’s value meets a specific criterion. You can apply conditions using logical operators and filtering techniques.\n",
    "\n",
    "Filtering with Boolean Conditions\n",
    "Use boolean expressions to filter rows based on conditions.\n",
    "You can combine multiple conditions using logical operators like & (AND), | (OR), and ~ (NOT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'Age' is greater than 30\n",
    "df_filtered = df[df['Age'] > 30]\n",
    "print(df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering Rows Based on Multiple Conditions\n",
    "\n",
    "# Filter rows where 'Age' is greater than 30 and 'Salary' is greater than 60000\n",
    "df_filtered_multi = df[(df['Age'] > 30) & (df['Salary'] > 60000)]\n",
    "print(df_filtered_multi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying Data\n",
    "The query() method provides a more efficient and readable way to filter a DataFrame based on string-based conditions. It’s especially useful when working with large datasets or complex filtering conditions.\n",
    "\n",
    "query()\n",
    "Allows you to filter data using a SQL-like syntax.\n",
    "Supports boolean expressions, comparisons, and even string operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query rows where 'Age' is greater than 30\n",
    "df_query = df.query('Age > 30')\n",
    "print(df_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Querying with Multiple Conditions\n",
    "\n",
    "# Query rows where 'Age' is greater than 30 and 'Salary' is less than 80000\n",
    "df_query_multi = df.query('Age > 30 and Salary < 80000')\n",
    "print(df_query_multi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregation and Grouping\n",
    "Aggregation and grouping are critical techniques in data analysis, allowing you to summarize and compute statistics over groups of data. Pandas provides powerful tools like groupby(), pivot_table(), and multi-level indexing to work with grouped data efficiently.\n",
    "<br>\n",
    " Grouping Data\n",
    "The groupby() function is used to group data based on one or more columns and apply aggregation functions like sum(), mean(), count(), and more. Grouping is often the first step before performing aggregations or transformations on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping by a Single Column and Applying Aggregation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'Department': ['HR', 'HR', 'IT', 'IT', 'Finance', 'Finance'],\n",
    "        'Employee': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank'],\n",
    "        'Salary': [50000, 60000, 70000, 80000, 90000, 100000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Group by 'Department' and calculate the total salary\n",
    "df_grouped = df.groupby('Department')['Salary'].sum()\n",
    "print(df_grouped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping by Multiple Columns and Applying Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Department' and 'Employee' and calculate the mean salary\n",
    "df_grouped_multi = df.groupby(['Department', 'Employee']).mean()\n",
    "print(df_grouped_multi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Multiple Aggregations at Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying multiple aggregation functions: sum and mean\n",
    "df_agg = df.groupby('Department')['Salary'].agg(['sum', 'mean'])\n",
    "print(df_agg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot Table with Multiple Indexes and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pivot table with 'Department' as index and 'Employee' as columns\n",
    "pivot_complex = df.pivot_table(values='Salary', index='Department', columns='Employee', aggfunc='sum')\n",
    "print(pivot_complex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-level Indexing (Hierarchical Indexing)\n",
    "Pandas supports multi-level indexing, also known as hierarchical indexing, which allows you to work with data that has multiple levels of indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping Data with a Multi-level Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Department' and 'Employee', and calculate the sum of salaries\n",
    "df_multi_index = df.groupby(['Department', 'Employee'])['Salary'].sum()\n",
    "print(df_multi_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing Data from a Multi-level Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the salary of 'Eve' in the 'Finance' department\n",
    "print(df_multi_index.loc[('Finance', 'Eve')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Resetting the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reset = df_multi_index.reset_index()\n",
    "print(df_reset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Data\n",
    "Time series data refers to data that is indexed or organized by time intervals, such as daily stock prices, monthly sales, or annual temperatures. Pandas offers robust functionality to work with time series data, allowing you to parse dates, resample data, and shift time values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Dates and Times\n",
    "Pandas provides a convenient function pd.to_datetime() to parse and convert various date formats into datetime objects, making it easier to manipulate and analyze time-based data.\n",
    "\n",
    "pd.to_datetime()\n",
    "Converts a list or column of strings representing dates into datetime objects.\n",
    "This allows for date-based indexing and manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with date strings\n",
    "data = {'Date': ['2023-01-01', '2023-02-01', '2023-03-01'],\n",
    "        'Sales': [200, 300, 400]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert 'Date' column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Set 'Date' as the index\n",
    "df.set_index('Date', inplace=True)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampling and Frequency Conversion\n",
    "Resampling is the process of converting time series data from one frequency to another. For instance, you might want to aggregate daily data to monthly data or disaggregate yearly data to quarterly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with daily data\n",
    "data = {'Date': pd.date_range('2023-01-01', periods=10, freq='D'),\n",
    "        'Sales': [200, 300, 400, 350, 450, 600, 500, 700, 800, 900]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Resample data to 3-day intervals, calculating the sum\n",
    "df_resampled = df.resample('3D').sum()\n",
    "print(df_resampled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Data Frequency Without Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert daily data to a weekly frequency without aggregation\n",
    "df_weekly = df.asfreq('W')\n",
    "print(df_weekly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shifting Data\n",
    "Shifting data refers to moving time series data forward or backward by a specific number of periods.\n",
    " Shifting Data Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift sales data forward by 1 period (lead)\n",
    "df_shifted_forward = df['Sales'].shift(1)\n",
    "print(df_shifted_forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift sales data backward by 1 period (lag)\n",
    "df_shifted_backward = df['Sales'].shift(-1)\n",
    "print(df_shifted_backward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing for Machine Learning\n",
    "Data preprocessing is a crucial step in preparing data for machine learning models. It involves cleaning, transforming, and scaling the data to ensure that the models work effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling\n",
    "Feature scaling ensures that numerical features are on a similar scale, which is especially important for algorithms sensitive to the magnitude of features (e.g., gradient descent-based algorithms, k-nearest neighbors, etc.).\n",
    "\n",
    "Two common scaling techniques are:\n",
    "\n",
    "Normalization (Min-Max Scaling): Scales values to a range between 0 and 1.\n",
    "Standardization (Z-Score Scaling): Centers the data to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'Age': [25, 35, 45, 55, 65],\n",
    "        'Salary': [30000, 50000, 70000, 90000, 110000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Min-Max Scaling (Manual Calculation)\n",
    "df['Age_scaled'] = (df['Age'] - df['Age'].min()) / (df['Age'].max() - df['Age'].min())\n",
    "df['Salary_scaled'] = (df['Salary'] - df['Salary'].min()) / (df['Salary'].max() - df['Salary'].min())\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[['Age_scaled', 'Salary_scaled']] = scaler.fit_transform(df[['Age', 'Salary']])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-Score Scaling (Standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[['Age_standardized', 'Salary_standardized']] = scaler.fit_transform(df[['Age', 'Salary']])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Encoding Categorical Variables\n",
    "Machine learning models require numerical inputs, so categorical variables need to be converted into numbers. This can be done using techniques such as One-Hot Encoding and Label Encoding.\n",
    "\n",
    "One-Hot Encoding\n",
    "One-hot encoding converts categorical variables into a binary column for each category. This can be done using the get_dummies() function in pandas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with categorical data\n",
    "data = {'City': ['New York', 'Los Angeles', 'Chicago', 'New York'],\n",
    "        'Salary': [50000, 60000, 55000, 65000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "df_encoded = pd.get_dummies(df, columns=['City'])\n",
    "print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoding\n",
    "Label encoding assigns a unique integer to each category. It’s simpler but can sometimes introduce unintended ordinal relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply Label Encoding to the 'City' column\n",
    "df['City_encoded'] = le.fit_transform(df['City'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Data for Training and Testing\n",
    "Splitting the dataset into training and testing sets is essential for model evaluation. The training set is used to train the model, and the testing set is used to evaluate its performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use train_test_split() from sklearn.model_selection to split the data.\n",
    "\n",
    "train_test_split()\n",
    "This function splits the data into training and testing sets, based on a specified ratio (e.g., 80% training, 20% testing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample DataFrame with features and target\n",
    "data = {'Age': [25, 35, 45, 55, 65],\n",
    "        'Salary': [30000, 50000, 70000, 90000, 110000],\n",
    "        'Purchased': [0, 1, 0, 1, 1]}  # Target variable\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features (X) and target (y)\n",
    "X = df[['Age', 'Salary']]\n",
    "y = df['Purchased']\n",
    "\n",
    "# Split data into 80% training and 20% testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set:\\n\", X_train)\n",
    "print(\"\\nTesting set:\\n\", X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " File I/O with pandas\n",
    "Pandas provides versatile functions to easily read data from various file formats (like CSV, Excel, JSON, SQL databases) and also write DataFrames back to these formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Data from Files\n",
    "Pandas makes it simple to load data from multiple file types, such as CSV, Excel, JSON, and SQL databases, using functions like read_csv(), read_excel(), read_json(), and read_sql()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data from a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from a CSV file\n",
    "df = pd.read_csv('Details.csv')\n",
    "\n",
    "# Display the first 5 rows of the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data from an Excel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from an Excel file (specifying the sheet name)\n",
    "df = pd.read_excel('Sample_data.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data from a JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from a JSON file\n",
    "df = pd.read_json('Sample_json.json')\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data from a SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mysql-connector-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to MySQL\n",
    "connection = mysql.connector.connect(\n",
    "    host=\"localhost\",  # Replace with your MySQL host\n",
    "    user=\"root\",  # Replace with your MySQL username\n",
    "    password=\"root\",  # Replace with your MySQL password\n",
    "    database=\"company\"  # Replace with your MySQL database\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Execute a SQL query to fetch data\n",
    "query = \"SELECT * FROM company \"  # Adjust the query based on your table and database\n",
    "cursor.execute(query)\n",
    "\n",
    "# Fetch all rows from the executed query\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Get the column names from the cursor's description attribute\n",
    "columns = [desc[0] for desc in cursor.description]\n",
    "\n",
    "# Convert the fetched data into a Pandas DataFrame\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Closing the cursor and connection\n",
    "cursor.close()\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Data to an Excel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('output.xlsx', index=False, sheet_name='Sheet1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Data to a JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a JSON file\n",
    "df.to_json('output.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Data to a SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to MySQL\n",
    "connection = mysql.connector.connect(\n",
    "    host=\"localhost\",  # Replace with your MySQL host\n",
    "    user=\"root\",  # Replace with your MySQL username\n",
    "    password=\"root\",  # Replace with your MySQL password\n",
    "    database=\"company\"  # Replace with your MySQL database\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Execute a SQL query to fetch data\n",
    "query = \"SELECT * FROM company where company_id = 1 \"  # Adjust the query based on your table and database\n",
    "cursor.execute(query)\n",
    "\n",
    "# Fetch all rows from the executed query\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Get the column names from the cursor's description attribute\n",
    "columns = [desc[0] for desc in cursor.description]\n",
    "\n",
    "# Convert the fetched data into a Pandas DataFrame\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "# MySQL connection details\n",
    "user = 'root'  # MySQL username\n",
    "password = 'root'  # MySQL password\n",
    "host = 'localhost'  # MySQL host, e.g., 'localhost' or an IP\n",
    "database = 'company'  # Name of the MySQL database\n",
    "\n",
    "# Create a SQLAlchemy engine for the connection\n",
    "engine = create_engine(f\"mysql+pymysql://{user}:{password}@{host}/{database}\")\n",
    "\n",
    "# Save the DataFrame to a MySQL table (table name = 'users')\n",
    "df.to_sql('users', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(\"DataFrame has been successfully saved to the 'users' table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
